# ğŸ”¥ FLARE24 Solution

This repository is the official implementation of [A 3D Unsupervised Domain Adaption Framework Combining Style Translation and Self-Training for Abdominal Organs Segmentation](https://openreview.net/forum?id=oSbUYnIDs9&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DMICCAI.org%2F2024%2FChallenge%2FFLARE%2FAuthors%23your-submissions)) of Team tju_vil_pioneers on FLARE24 challenge.

## ğŸ” Overview
This work addresses the challenge of adapting CT-trained segmentation models to MR images by generating synthetic MR data, employing self-training strategies, and a two-stage segmentation framework in the FLARE24 dataset. For more details, see the pipeline diagram below:

<div align=center>
<img src="./assets/pipeline.png" alt="Pipeline" width="800"/>
</div>

## âš™ï¸ Environments and Requirements
* Ubuntu 20.04.6 LTS
* Intel(R) Xeon(R) Platinum 8153 CPU @ 2.00GHz, RAM 192GB , NVIDIA Tesla V100 (32G)
* CUDA >= 11.3
* python >= 3.7.13

To set up the environment, follow these steps:

```
conda create -n FLARE24 python=3.7.13
conda activate FLARE24
pip install -r requirements.txt
```
> PS:  should install torch==1.12.0+cu113 before pip install requirement. 
```
pip install torch==1.12.0+cu113 --extra-index-url https://download.pytorch.org/whl/cu113
```
## ğŸ’¾ Dataset
The training Data and validation data are provided by the [FLARE24](https://www.codabench.org/competitions/2296/). In short, there are 2200 partial labeled CT and ? unlabeled MR data for training, 100 public cases for validation and 200 hidden cases for the final test.

æˆ‘ä»¬çš„æ•°æ®ç›®å½•ç»“æ„å¦‚ä¸‹å›¾
TODO: è¡¥å……

## ğŸª„ Preprocessing

æˆ‘ä»¬æ ¹æ®æ¨¡å‹è®­ç»ƒçš„é˜¶æ®µï¼ŒåŒ…å«äº†å¤šä¸ªæ­¥éª¤çš„é¢„å¤„ç†ã€‚
ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä»…ä»…ä½¿ç”¨CTæ•°æ®å’Œé¢†åŸŸè¿ç§»çš„æ•°æ®
For Step 1 Style Translation
```
python xxx
```
ç¬¬äºŒé˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿­ä»£ç”ŸæˆBig SegNetçš„ä¼ªæ ‡ç­¾ç”¨æ¥è¿›è¡Œself-train
For Step 2 self-Training Strategy
```
python xxx
```
Now you can train your models.
ç¬¬ä¸‰é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒå¥½çš„æ¨¡å‹ç”ŸæˆSmall SegNetçš„ä¼ªæ ‡ç­¾ï¼Œè¿›è¡Œæœ€ç»ˆæ¨¡å‹çš„è®­ç»ƒ
For Step 3 Two-Stage Segmentation
```
python xxxx
```
Now you can train your models.


# ğŸ–¥ï¸ Train

Training our model consists of ? main steps, as depicted in the pipeline figure above. Follow these instructions to conduct the training process effectively:

### Step 1: ä½³ç‡¨è¡¥å……ï¼Œ åŸŸè¿ç§»ç®—æ³•
Firstly, prepare a dataset of 250 images with 13-organ labels. Use this dataset to train a large organ model. 

```bash
nnUNetv2_train TASK_ID 3d_fullres all -tr STUNetTrainer_large
```

### Step 2: Generate Pseudo Labels for Step2 
With the model trained in Step 1, generate pseudo labels for 1494 tumor-annotated images.

```bash
nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d TASK_ID -c 3d_fullres -f all -tr STUNetTrainer_large
```

### Step 3: Merge Labels and Train Large Model
Merge the pseudo organ labels and actual tumor labels in 1497 images, and then train a large model using this merged dataset.

```bash
nnUNetv2_train TASK_ID 3d_fullres all -tr STUNetTrainer_large
```

### Step 4: Generate Pseudo Labels for All Images
Using the model trained in Step 3, generate pseudo labels for all 4000 images in the dataset.

```bash
nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d TASK_ID -c 3d_fullres -f all -tr STUNetTrainer_large
```

### Step 5: Train Small Model for Deployment
Finally, train a small model on the dataset of 4000 images. This model will be used for deployment.

```bash
nnUNetv2_train TASK_ID 3d_fullres all -tr STUNetTrainer_base
```

You can download trained models here:

TODO:è¡¥å……link trained on the above dataset with the above code.

2. To fine-tune the model on a customized dataset, run this command:

```bash
nnUNetv2_train TASK_ID 3d_fullres all -tr STUNetTrainer_base
```

## ğŸ—³ï¸ Inference

1. To infer the testing cases, run this command:

```bash
nnUNetv2_train TASK_ID 3d_fullres all -tr STUNetTrainer_base
```
2. Docker containers on [docker container_link](354)

Will be released soon.

## ğŸ“Š Evaluation

To compute the evaluation metrics, run:

```eval
python eval.py --seg_data <path_to_inference_results> --gt_data <path_to_ground_truth>
```

>Describe how to evaluate the inference results and obtain the reported results in the paper.



## ğŸ“‹ Results

Our method achieves the following performance on [FLARE24](https://www.codabench.org/competitions/2296/)

| Dataset Name       | DSC(%) | NSD(%) |
|--------------------|:------:|:------:|
| Validation Dataset | 79.42% | 86.46% |
| Test Dataset       |   ?    |   ï¼Ÿ    |




## ğŸ’• é¡¹ç›®æˆå‘˜ <a id="é¡¹ç›®æˆå‘˜"></a>
#TODO
- [xx](xx) 

## ğŸ–Šï¸ Citation <a id="Citation"></a>

```bibtex
todo
```

## Contributing

## ğŸ‰‘å¼€æºè®¸å¯è¯ <a id="å¼€æºè®¸å¯è¯"></a>

è¯¥é¡¹ç›®é‡‡ç”¨ [Apache License 2.0 å¼€æºè®¸å¯è¯](LICENSE) åŒæ—¶ï¼Œè¯·éµå®ˆæ‰€ä½¿ç”¨çš„æ¨¡å‹ä¸æ•°æ®é›†çš„è®¸å¯è¯ã€‚
## Acknowledgement

 We thank the contributors of [public FLARE24 datasets](?).

> We thank the contributors of public datasets. 